{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting markdownify\n",
      "  Downloading markdownify-0.12.1-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.9 in c:\\users\\amrabdel-motaleb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from markdownify) (4.12.3)\n",
      "Requirement already satisfied: six<2,>=1.15 in c:\\users\\amrabdel-motaleb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from markdownify) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\amrabdel-motaleb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from beautifulsoup4<5,>=4.9->markdownify) (2.5)\n",
      "Downloading markdownify-0.12.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: markdownify\n",
      "Successfully installed markdownify-0.12.1\n"
     ]
    }
   ],
   "source": [
    "%pip install markdownify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    # Fetching the HTML content\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Will raise an HTTPError for bad requests\n",
    "\n",
    "    # Parsing HTML\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Identify the main content - adjust the selector as needed\n",
    "    content_div = soup.find('div', {'id': 'main-content'})  # You can change 'div' and 'id' based on your needs\n",
    "\n",
    "    if content_div:\n",
    "        # Converting to Markdown\n",
    "        markdown_text = md(str(content_div), heading_style=\"ATX\")\n",
    "        return markdown_text\n",
    "    else:\n",
    "        return \"Content not found.\"\n",
    "\n",
    "# Example usage\n",
    "url = 'https://chatgpt.com/share/ce7b4735-b847-488c-8321-dded8821b38a'\n",
    "markdown_output = html_to_markdown(url)\n",
    "print(markdown_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Hello World | IT-Journey\"\n",
      "url: \"https://it-journey.dev/home/\"\n",
      "date_retrieved: \"2024-05-12 19:46:56\"\n",
      "description: \"This is an overall outline of this site, and a place to start or come back to when lost.\"\n",
      "---\n",
      "\n",
      "[root](/)\n",
      " \n",
      " / Hello World\n",
      " \n",
      " On this page\n",
      " \n",
      "* *On this page** \n",
      "\n",
      "* [Abstract](#abstract)\n",
      "* [Roadmap](#roadmap)\n",
      "\t+ [Zer0 to Her0 Quest](#zer0-to-her0-quest)\n",
      "\t+ [Skill Level Route](#skill-level-route)\n",
      "\t+ [Stack Attack](#stack-attack)\n",
      "\t+ [Specialization route](#specialization-route)\n",
      "\t+ [Project Route](#project-route)\n",
      "* [Quick Start](#quick-start)\n",
      "\t+ [Site layout](#site-layout)\n",
      "\n",
      "â†‘\n",
      "Back to Top\n",
      "\n",
      "# Hello World\n",
      "\n",
      "This is where we begin our journey. The place where we return after getting lost or wandering off.\n",
      "Think of this as our home base with a collection of maps, tools, and information we need to traverse through this chaotic digital world.\n",
      "There are journals to capture our experiences and findings, notes to quickly reference when our memories fail, and a library of documentation that gives us the depth of knowledge to build upon and share.\n",
      "Everything here is open source and free to use, and the goal is to make this repository a comprehensive learning tool for everyone to use and share.\n",
      "\n",
      "# Abstract\n",
      "\n",
      "From zero to hero collection of docs, tools, scripts, walk-through, and information to help with your IT journey.\n",
      "\n",
      "# Roadmap\n",
      "\n",
      "Here are a few routes:\n",
      "\n",
      "# Zer0 to Her0 [Quest](/quest/)\n",
      "\n",
      "1. Prologue - Init Hello World\n",
      "\t1. Who are you and what do you want?\n",
      "\t2. Are you willing to put in the time and effort to become a full-stack developer?\n",
      "\t3. What are your minimum requirements to start this journey?\n",
      "\t4. What are your goals?\n",
      "\t5. What are your expectations?\n",
      "2. Chapter 1 - Identity Crisis\n",
      "\t1. \n",
      "\t2. Minimum requirements\n",
      "\t3. Class Selection up\n",
      "\t4. Character Building\n",
      "3. Chapter 2 - Training\n",
      "\n",
      "# Skill Level Route\n",
      "\n",
      "For those who prefer the route based on difficulty:\n",
      "\n",
      "1. Beginner - 000\n",
      "2. Intermediate - 001\n",
      "3. Advanced - 010\n",
      "4. Expert - 011\n",
      "5. Master - 100\n",
      "6. Hero - 101\n",
      "7. Super Hero - 110\n",
      "8. God - 111\n",
      "\n",
      "# Stack Attack\n",
      "\n",
      "For those who are intermediate/advanced and want to work on a specific stack:\n",
      "\n",
      "1. Front-end (HTML, CSS, JS)\n",
      "2. Back-end (Python, PHP, Ruby, Command Line)\n",
      "3. Databases (MySQL, NoSQL, PostgreSQL)\n",
      "4. Integrations (API, REST, GraphQL, SOAP)\n",
      "5. Infrastructure (AWS, Azure, GCP, Linux)\n",
      "6. Solutions (LAMP, Jamstack, MERN, WINS)\n",
      "\n",
      "[Source](https://devopedia.org/full-stack-developer)\n",
      "[Wiki](https://en.wikipedia.org/wiki/Solution_stack)\n",
      "\n",
      "# Specialization route\n",
      "\n",
      "For those who are advanced/expert and what to specialize in a field:\n",
      "\n",
      "1. Infrastructure (System Administration, Networking, Operating Systems)\n",
      "2. DevOps (Source Code control, CD/CI, Automated Testing, Agile Development)\n",
      "3. Web Design (HTML, CSS, JavaScript, Jekyll, Bootstrap)\n",
      "4. Software Engineering (Python, Ruby, Java, C#, C++)\n",
      "5. Data Science (Python, SQL, BI, Hadoop, Spark, HBase, Hive, Kafka, Cassandra)\n",
      "6. Cyber Security (Firewalls, Metadata analysis, Network Penetration, Ransomware, Vulnerability, Exploits, Malware)\n",
      "7. Multimedia/Graphic Design (GIMP, Blender, Inkscape, Krita, Pencil 2D)\n",
      "8. Mobile Development (Android, iOS, React Native)\n",
      "9. Game Development (Unity, Unreal Engine)\n",
      "10. AI (Machine Learning, Deep Learning, Natural Language Processing)\n",
      "\n",
      "# Project Route\n",
      "\n",
      "1. Wikimedia Server - [Downloads](https://www.mediawiki.org/wiki/Download)\n",
      "2. Personal Website\n",
      "3. Retro Picade\n",
      "4. Mobile App\n",
      "5. Web Scraper\n",
      "6. Documentation Site\n",
      "7. Video game\n",
      "\n",
      "# Quick Start\n",
      "\n",
      "For those who are already familiar with core IT concepts, this is the quick start guide to get you going. There are some prerequisites listed before you can clone this repository. Each is linked to a detailed installation instruction.\n",
      "\n",
      "Master Setup [Local link](\\_quickstart\\machine-setup.md) Or [Web link](\\quickstart\\machine-setup\\)\n",
      "\n",
      "Integrated Development Environment (Visual Studio Code) [Local link](\\_quickstart\\vscode.md) Or [Web link](\\quickstart\\vscode\\)\n",
      "\n",
      "Static Website Generator (Jekyll) [Local link](\\_quickstart\\Jekyll.md) Or [Web link](\\quickstart\\Jekyll\\)\n",
      "\n",
      "# Site layout\n",
      "\n",
      "# Top Navigation Bar\n",
      "\n",
      "![](../assets/images/top-nav.png)\n",
      "\n",
      "```\n",
      "\n",
      "<div class=\"bd-example-snippet bd-code-snippet\">\n",
      "{%- include header.html -%}\n",
      "</div>\n",
      "\n",
      "```\n",
      "\n",
      "This is a fixed navigation bar that is always visible at the top of the page. It is a horizontal bar that contains links to the different sections of the site. The links are organized into three sections:\n",
      "\n",
      "* [Journals](/posts/)\n",
      "* [Library](/docs/)\n",
      "* [Notes](/Notes/)\n",
      "\n",
      "# Sidebar Navigation\n",
      "\n",
      "The sidebar navigation is a vertical bar that is always visible on the left side of the page. It is automatically generated based on the navigation YAML file under ../\\_data/navigation.yml.\n",
      "\n",
      "[Including](https://jekyllrb.com/docs/includes/) a [truncated](https://shopify.github.io/liquid/filters/truncate/) navigation YAML file under the `_data` folder will automatically generate the sidebar and top navigation.\n",
      "\n",
      "```\n",
      "{% capture nav %}{% include_relative _data/navigation.yml %}{% endcapture %}{{ nav | truncate: 332 }}\n",
      "\n",
      "```\n",
      "\n",
      "# Table Of Contents right sidebar\n",
      "\n",
      "This is an automatically generated table of contents that is always visible on the right side of the page. It is generated from using a programs located under /\\_includes/toc.html\n",
      "\n",
      "It is based on the heading tags in the markdown file. The table of contents is generated from the markdown file and is updated whenever the markdown file is updated.\n",
      "\n",
      "# Source Code Short Cuts\n",
      "\n",
      "Above the right TOC, there is a short cut to the source code. This is a link to the GitHub repository. The link to the shortcut is based on the config file located under /\\_config.yml. This is the where you have forked this repository.\n",
      "\n",
      "```\n",
      "repository               : \"bamr87/it-journey\" # GitHub username/repo-name\n",
      "local_repo               : \"it-journey\"\n",
      "home_dir_pc              : &home-win '$HOME'\n",
      "home_dir_mac             : &home-mac '$HOME'\n",
      "local_git_pc             : [ *home-win, 'github\\' ]\n",
      "local_git_mac            : [ *home-mac, 'GitHub/' ]\n",
      "\n",
      "```\n",
      "\n",
      "NOTE: Replace `$HOME` with your home directory. Normally, it is the user id of the machine. Just type `echo $HOME` in the terminal.\n",
      "\n",
      "# Posts\n",
      "\n",
      "* Mar 11, 2024\n",
      "# [GPT - CV Analysis](/posts/2024-03-11-cv-analysis/)\n",
      "* Feb 14, 2024\n",
      "# [Building a VS Code Extension](/posts/2024-02-14-vscode-extension/)\n",
      "* Feb 10, 2024\n",
      "# [RetroPie Imaging](/posts/2024-02-10-retropie-imaging/)\n",
      "* Dec 4, 2023\n",
      "# [Jekyll - SEO](/posts/2023-12-06-jekyll-seo/)\n",
      "* Dec 4, 2023\n",
      "# [robots dot txt](/posts/2023-12-04-robots-dot-txt/)\n",
      "* Nov 4, 2023\n",
      "# [LaTex your CV](/posts/2023-11-04-latex-your-cv/)\n",
      "* Apr 20, 2023\n",
      "# [Arab americans](/posts/2023-04-20-arab-americans/)\n",
      "* Apr 14, 2023\n",
      "# [Windows Sub-linux Setup](/posts/2023-04-14-windows-sub-linux-setu/)\n",
      "* Apr 14, 2023\n",
      "# [frontmatter generator](/posts/2023-04-14-frontmatter-generator/)\n",
      "* Apr 7, 2023\n",
      "# [Krita Tips and Tricks](/posts/2023-04-07-krita-tips-and-tricks/)\n",
      "* Mar 26, 2023\n",
      "# [Chat GPT vs The Impossible Triangle](/posts/2023-03-26-chat-gpt-vs-the-impossible-triangle/)\n",
      "* Mar 17, 2023\n",
      "# [penrose triangle](/posts/2023-03-17-penrose-triangle/)\n",
      "* Dec 29, 2022\n",
      "# [linux ftp server](/posts/2022-12-29-linux-ftp-server/)\n",
      "* Dec 14, 2022\n",
      "# [Enabling docker on azure via VSCode](/posts/2022-12-14-enabling-docker-on-azure-via-vscode/)\n",
      "* Dec 5, 2022\n",
      "# [sharex](/posts/2022-12-05-sharex/)\n",
      "* Jul 24, 2022\n",
      "# [zer0-mistakes-bootstrap](/posts/2022-07-24-zer0-mistakes-bootstrap/)\n",
      "* Jul 16, 2022\n",
      "# [zer0-mistakes with github-pages and jekyll](/posts/2022-07-16-zer0-mistakes/)\n",
      "* Jul 1, 2022\n",
      "# [Angular Tour of Heros](/posts/2022-07-01-angular-tour-of-heros/)\n",
      "* Jun 19, 2022\n",
      "# [Webflow to Github Pages - Migration](/posts/2022-06-19-webflow-to-github-pages-migration/)\n",
      "* Jun 16, 2022\n",
      "# [GIthub Pages Custom Domain](/posts/2022-06-16-github-pages-custom-domain/)\n",
      "* Jun 10, 2022\n",
      "# [Desktop Widgets - Windows](/posts/2022-06-10-desktop-widgets-windows/)\n",
      "* Jun 4, 2022\n",
      "# [Add-Library-Category](/posts/2022-06-04-add-library-category/)\n",
      "* May 21, 2022\n",
      "# [Foundations - 000](/posts/2022-05-21-foundations-000/)\n",
      "* May 21, 2022\n",
      "# [fdsa](/posts/2022-05-21-fdsa/)\n",
      "* May 9, 2022\n",
      "# [test](/posts/js-test/)\n",
      "* Feb 27, 2022\n",
      "# [dual boot win linux](/posts/2022-02-27-dual-boot-win-linux/)\n",
      "* Feb 26, 2022\n",
      "# [android app](/posts/2022-02-26-android-app/)\n",
      "* Jan 23, 2022\n",
      "# [Winget your apps](/posts/2022-01-23-winget-your-apps/)\n",
      "* Jan 9, 2022\n",
      "# [Sonic Pi](/posts/2022-01-09-sonic-pi/)\n",
      "* Jan 7, 2022\n",
      "# [Fish Flavored Egg plant](/posts/2022-01-07-fish-flavored-egg-plant/)\n",
      "* Jan 7, 2022\n",
      "# [Jekyll and Travis](/posts/2022-01-07-jekyll-and-travis/)\n",
      "* Jan 5, 2022\n",
      "# [Angolia-Jekyll Setup With Github Pages](/posts/2022-01-05-angolia-jekyll-setup-with-github-pages/)\n",
      "* Dec 29, 2021\n",
      "# [Getting Started with Cloud Computing](/posts/2021-12-29-getting-started-with-cloud-computing/)\n",
      "* Nov 8, 2021\n",
      "# [IT Purpose](/posts/purpose/)\n",
      "* Oct 27, 2021\n",
      "# [Build-Destroy-Repeat](/posts/bdr/)\n",
      "* Sep 26, 2021\n",
      "# [Welcome to Jekyll!](/posts/2021-09-26-welcome-to-jekyll/)\n",
      "* Aug 22, 2019\n",
      "# [Auto Hide Navbar](/posts/2024-03-06-auto-hide-nav-bar/)\n",
      "* Jan 1, 2000\n",
      "# [posts](/posts/)\n",
      "* Jan 1, 2000\n",
      "# [articles](/posts/articles)\n",
      "\n",
      "subscribe [via RSS](/feed.xml)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from datetime import datetime\n",
    "\n",
    "def find_content(soup):\n",
    "    content_selectors = [\n",
    "        {'tag': 'div', 'attr': {'class': 'main-content'}},\n",
    "        {'tag': 'main', 'attr': {}},\n",
    "        {'tag': 'article', 'attr': {}}\n",
    "    ]\n",
    "    for selector in content_selectors:\n",
    "        content = soup.find(selector['tag'], attrs=selector['attr'])\n",
    "        if content:\n",
    "            return content\n",
    "    return soup.find('body') or None\n",
    "\n",
    "def extract_metadata(soup, url):\n",
    "    title = soup.find('title').text if soup.find('title') else 'No Title'\n",
    "    description_tag = soup.find('meta', attrs={\"name\": \"description\"})\n",
    "    description = description_tag['content'] if description_tag else 'No Description'\n",
    "    date_retrieved = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return {\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'date_retrieved': date_retrieved,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "def clean_markdown(markdown_text):\n",
    "    lines = markdown_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    consecutive_blank_lines = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() == '':\n",
    "            consecutive_blank_lines += 1\n",
    "            if consecutive_blank_lines > 1:\n",
    "                continue\n",
    "        else:\n",
    "            consecutive_blank_lines = 0\n",
    "\n",
    "        if line.startswith('#'):\n",
    "            # Ensure space after '#' in headings\n",
    "            cleaned_line = line.replace('#', '').strip()\n",
    "            line = '#' + ' ' + cleaned_line\n",
    "        elif line.startswith(('-', '*', '+')) and not line.startswith(('---', '***')):\n",
    "            # Ensure space after list markers\n",
    "            if line[1] != ' ':\n",
    "                line = line[0] + ' ' + line[1:]\n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    content_div = find_content(soup)\n",
    "    metadata = extract_metadata(soup, url)\n",
    "\n",
    "    if content_div:\n",
    "        raw_markdown = md(str(content_div), heading_style=\"ATX\")\n",
    "        markdown_text = clean_markdown(raw_markdown)\n",
    "\n",
    "        frontmatter = f\"\"\"---\n",
    "title: \"{metadata['title']}\"\n",
    "url: \"{metadata['url']}\"\n",
    "date_retrieved: \"{metadata['date_retrieved']}\"\n",
    "description: \"{metadata['description']}\"\n",
    "---\n",
    "\"\"\"\n",
    "        return frontmatter + markdown_text\n",
    "    else:\n",
    "        return \"Content not found.\"\n",
    "\n",
    "# Example usage\n",
    "url = 'https://it-journey.dev/home/'\n",
    "markdown_output = html_to_markdown(url)\n",
    "print(markdown_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"ChatGPT\"\n",
      "url: \"https://chatgpt.com/share/ce7b4735-b847-488c-8321-dded8821b38a\"\n",
      "date_retrieved: \"2024-05-12 22:09:26\"\n",
      "description: \"ChatGPT is a free-to-use AI system. Use it for engaging conversations, gain insights, automate tasks, and witness the future of AI, all in one place.\"\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from datetime import datetime\n",
    "\n",
    "def find_content(soup):\n",
    "    content_selectors = [\n",
    "        {'tag': 'div', 'attr': {'class': 'main-content'}},\n",
    "        {'tag': 'main', 'attr': {}},\n",
    "        {'tag': 'article', 'attr': {}}\n",
    "    ]\n",
    "    for selector in content_selectors:\n",
    "        content = soup.find(selector['tag'], attrs=selector['attr'])\n",
    "        if content:\n",
    "            return content\n",
    "    return soup.find('body') or None\n",
    "\n",
    "def extract_metadata(soup, url):\n",
    "    title = soup.find('title').text if soup.find('title') else 'No Title'\n",
    "    description_tag = soup.find('meta', attrs={\"name\": \"description\"})\n",
    "    description = description_tag['content'] if description_tag else 'No Description'\n",
    "    date_retrieved = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return {\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'date_retrieved': date_retrieved,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "def clean_markdown(markdown_text):\n",
    "    lines = markdown_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    consecutive_blank_lines = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() == '':\n",
    "            consecutive_blank_lines += 1\n",
    "            if consecutive_blank_lines > 1:\n",
    "                continue\n",
    "        else:\n",
    "            consecutive_blank_lines = 0\n",
    "\n",
    "        # Correct handling for headers\n",
    "        if line.startswith('#'):\n",
    "            # Ensure space after '#' in all levels of headings\n",
    "            header_count = len(line) - len(line.lstrip('#'))  # Count the number of '#'\n",
    "            if line[header_count] != ' ':\n",
    "                line = line[:header_count] + ' ' + line[header_count:].strip()\n",
    "\n",
    "        elif line.startswith(('-', '*', '+')) and not line.startswith(('---', '***')):\n",
    "            # Ensure space after list markers\n",
    "            if line[1] != ' ':\n",
    "                line = line[0] + ' ' + line[1:]\n",
    "        \n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    content_div = find_content(soup)\n",
    "    metadata = extract_metadata(soup, url)\n",
    "\n",
    "    if content_div:\n",
    "        raw_markdown = md(str(content_div), heading_style=\"ATX\")\n",
    "        markdown_text = clean_markdown(raw_markdown)\n",
    "\n",
    "        frontmatter = f\"\"\"---\n",
    "title: \"{metadata['title']}\"\n",
    "url: \"{metadata['url']}\"\n",
    "date_retrieved: \"{metadata['date_retrieved']}\"\n",
    "description: \"{metadata['description']}\"\n",
    "---\n",
    "\"\"\"\n",
    "        return frontmatter + markdown_text\n",
    "    else:\n",
    "        return \"Content not found.\"\n",
    "\n",
    "# Example usage\n",
    "url = 'https://chatgpt.com/share/ce7b4735-b847-488c-8321-dded8821b38a'\n",
    "markdown_output = html_to_markdown(url)\n",
    "print(markdown_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"Hello World | IT-Journey\"\n",
      "url: \"https://it-journey.dev/home/\"\n",
      "date_retrieved: \"2024-05-12 21:55:32\"\n",
      "description: \"This is an overall outline of this site, and a place to start or come back to when lost.\"\n",
      "---\n",
      "\n",
      "[root](/)\n",
      " \n",
      " / Hello World\n",
      " \n",
      " On this page\n",
      " \n",
      "**On this page** \n",
      "\n",
      "* [Abstract](#abstract)\n",
      "* [Roadmap](#roadmap)\n",
      "\t+ [Zer0 to Her0 Quest](#zer0-to-her0-quest)\n",
      "\t+ [Skill Level Route](#skill-level-route)\n",
      "\t+ [Stack Attack](#stack-attack)\n",
      "\t+ [Specialization route](#specialization-route)\n",
      "\t+ [Project Route](#project-route)\n",
      "* [Quick Start](#quick-start)\n",
      "\t+ [Site layout](#site-layout)\n",
      "\n",
      "â†‘\n",
      "Back to Top\n",
      "\n",
      "Hello World\n",
      "===========\n",
      "\n",
      "This is where we begin our journey. The place where we return after getting lost or wandering off.\n",
      "Think of this as our home base with a collection of maps, tools, and information we need to traverse through this chaotic digital world.\n",
      "There are journals to capture our experiences and findings, notes to quickly reference when our memories fail, and a library of documentation that gives us the depth of knowledge to build upon and share.\n",
      "Everything here is open source and free to use, and the goal is to make this repository a comprehensive learning tool for everyone to use and share.\n",
      "\n",
      "Abstract\n",
      "--------\n",
      "\n",
      "From zero to hero collection of docs, tools, scripts, walk-through, and information to help with your IT journey.\n",
      "\n",
      "Roadmap\n",
      "-------\n",
      "\n",
      "Here are a few routes:\n",
      "\n",
      "### Zer0 to Her0 [Quest](/quest/)\n",
      "\n",
      "1. Prologue - Init Hello World\n",
      "\t1. Who are you and what do you want?\n",
      "\t2. Are you willing to put in the time and effort to become a full-stack developer?\n",
      "\t3. What are your minimum requirements to start this journey?\n",
      "\t4. What are your goals?\n",
      "\t5. What are your expectations?\n",
      "2. Chapter 1 - Identity Crisis\n",
      "\t1. \n",
      "\t2. Minimum requirements\n",
      "\t3. Class Selection up\n",
      "\t4. Character Building\n",
      "3. Chapter 2 - Training\n",
      "\n",
      "### Skill Level Route\n",
      "\n",
      "For those who prefer the route based on difficulty:\n",
      "\n",
      "1. Beginner - 000\n",
      "2. Intermediate - 001\n",
      "3. Advanced - 010\n",
      "4. Expert - 011\n",
      "5. Master - 100\n",
      "6. Hero - 101\n",
      "7. Super Hero - 110\n",
      "8. God - 111\n",
      "\n",
      "### Stack Attack\n",
      "\n",
      "For those who are intermediate/advanced and want to work on a specific stack:\n",
      "\n",
      "1. Front-end (HTML, CSS, JS)\n",
      "2. Back-end (Python, PHP, Ruby, Command Line)\n",
      "3. Databases (MySQL, NoSQL, PostgreSQL)\n",
      "4. Integrations (API, REST, GraphQL, SOAP)\n",
      "5. Infrastructure (AWS, Azure, GCP, Linux)\n",
      "6. Solutions (LAMP, Jamstack, MERN, WINS)\n",
      "\n",
      "[Source](https://devopedia.org/full-stack-developer)\n",
      "[Wiki](https://en.wikipedia.org/wiki/Solution_stack)\n",
      "\n",
      "### Specialization route\n",
      "\n",
      "For those who are advanced/expert and what to specialize in a field:\n",
      "\n",
      "1. Infrastructure (System Administration, Networking, Operating Systems)\n",
      "2. DevOps (Source Code control, CD/CI, Automated Testing, Agile Development)\n",
      "3. Web Design (HTML, CSS, JavaScript, Jekyll, Bootstrap)\n",
      "4. Software Engineering (Python, Ruby, Java, C#, C++)\n",
      "5. Data Science (Python, SQL, BI, Hadoop, Spark, HBase, Hive, Kafka, Cassandra)\n",
      "6. Cyber Security (Firewalls, Metadata analysis, Network Penetration, Ransomware, Vulnerability, Exploits, Malware)\n",
      "7. Multimedia/Graphic Design (GIMP, Blender, Inkscape, Krita, Pencil 2D)\n",
      "8. Mobile Development (Android, iOS, React Native)\n",
      "9. Game Development (Unity, Unreal Engine)\n",
      "10. AI (Machine Learning, Deep Learning, Natural Language Processing)\n",
      "\n",
      "### Project Route\n",
      "\n",
      "1. Wikimedia Server - [Downloads](https://www.mediawiki.org/wiki/Download)\n",
      "2. Personal Website\n",
      "3. Retro Picade\n",
      "4. Mobile App\n",
      "5. Web Scraper\n",
      "6. Documentation Site\n",
      "7. Video game\n",
      "\n",
      "Quick Start\n",
      "-----------\n",
      "\n",
      "For those who are already familiar with core IT concepts, this is the quick start guide to get you going. There are some prerequisites listed before you can clone this repository. Each is linked to a detailed installation instruction.\n",
      "\n",
      "Master Setup [Local link](\\_quickstart\\machine-setup.md) Or [Web link](\\quickstart\\machine-setup\\)\n",
      "\n",
      "Integrated Development Environment (Visual Studio Code) [Local link](\\_quickstart\\vscode.md) Or [Web link](\\quickstart\\vscode\\)\n",
      "\n",
      "Static Website Generator (Jekyll) [Local link](\\_quickstart\\Jekyll.md) Or [Web link](\\quickstart\\Jekyll\\)\n",
      "\n",
      "### Site layout\n",
      "\n",
      "#### Top Navigation Bar\n",
      "\n",
      "![](../assets/images/top-nav.png)\n",
      "\n",
      "```\n",
      "<div class=\"bd-example-snippet bd-code-snippet\">\n",
      "{%- include header.html -%}\n",
      "</div>\n",
      "```\n",
      "\n",
      "This is a fixed navigation bar that is always visible at the top of the page. It is a horizontal bar that contains links to the different sections of the site. The links are organized into three sections:\n",
      "\n",
      "* [Journals](/posts/)\n",
      "* [Library](/docs/)\n",
      "* [Notes](/Notes/)\n",
      "\n",
      "#### Sidebar Navigation\n",
      "\n",
      "The sidebar navigation is a vertical bar that is always visible on the left side of the page. It is automatically generated based on the navigation YAML file under ../\\_data/navigation.yml.\n",
      "\n",
      "[Including](https://jekyllrb.com/docs/includes/) a [truncated](https://shopify.github.io/liquid/filters/truncate/) navigation YAML file under the _data folder will automatically generate the sidebar and top navigation.\n",
      "\n",
      "```\n",
      "{% capture nav %}{% include_relative _data/navigation.yml %}{% endcapture %}{{ nav | truncate: 332 }}\n",
      "```\n",
      "\n",
      "#### Table Of Contents right sidebar\n",
      "\n",
      "This is an automatically generated table of contents that is always visible on the right side of the page. It is generated from using a programs located under /\\_includes/toc.html\n",
      "\n",
      "It is based on the heading tags in the markdown file. The table of contents is generated from the markdown file and is updated whenever the markdown file is updated.\n",
      "\n",
      "#### Source Code Short Cuts\n",
      "\n",
      "Above the right TOC, there is a short cut to the source code. This is a link to the GitHub repository. The link to the shortcut is based on the config file located under /\\_config.yml. This is the where you have forked this repository.\n",
      "\n",
      "```\n",
      "repository               : \"bamr87/it-journey\" # GitHub username/repo-name\n",
      "local_repo               : \"it-journey\"\n",
      "home_dir_pc              : &home-win '$HOME'\n",
      "home_dir_mac             : &home-mac '$HOME'\n",
      "local_git_pc             : [ *home-win, 'github\\' ]\n",
      "local_git_mac            : [ *home-mac, 'GitHub/' ]\n",
      "```\n",
      "\n",
      "NOTE: Replace $HOME with your home directory. Normally, it is the user id of the machine. Just type echo $HOME in the terminal.\n",
      "\n",
      "Posts\n",
      "-----\n",
      "\n",
      "* Mar 11, 2024\n",
      "### [GPT - CV Analysis](/posts/2024-03-11-cv-analysis/)\n",
      "* Feb 14, 2024\n",
      "### [Building a VS Code Extension](/posts/2024-02-14-vscode-extension/)\n",
      "* Feb 10, 2024\n",
      "### [RetroPie Imaging](/posts/2024-02-10-retropie-imaging/)\n",
      "* Dec 4, 2023\n",
      "### [Jekyll - SEO](/posts/2023-12-06-jekyll-seo/)\n",
      "* Dec 4, 2023\n",
      "### [robots dot txt](/posts/2023-12-04-robots-dot-txt/)\n",
      "* Nov 4, 2023\n",
      "### [LaTex your CV](/posts/2023-11-04-latex-your-cv/)\n",
      "* Apr 20, 2023\n",
      "### [Arab americans](/posts/2023-04-20-arab-americans/)\n",
      "* Apr 14, 2023\n",
      "### [Windows Sub-linux Setup](/posts/2023-04-14-windows-sub-linux-setu/)\n",
      "* Apr 14, 2023\n",
      "### [frontmatter generator](/posts/2023-04-14-frontmatter-generator/)\n",
      "* Apr 7, 2023\n",
      "### [Krita Tips and Tricks](/posts/2023-04-07-krita-tips-and-tricks/)\n",
      "* Mar 26, 2023\n",
      "### [Chat GPT vs The Impossible Triangle](/posts/2023-03-26-chat-gpt-vs-the-impossible-triangle/)\n",
      "* Mar 17, 2023\n",
      "### [penrose triangle](/posts/2023-03-17-penrose-triangle/)\n",
      "* Dec 29, 2022\n",
      "### [linux ftp server](/posts/2022-12-29-linux-ftp-server/)\n",
      "* Dec 14, 2022\n",
      "### [Enabling docker on azure via VSCode](/posts/2022-12-14-enabling-docker-on-azure-via-vscode/)\n",
      "* Dec 5, 2022\n",
      "### [sharex](/posts/2022-12-05-sharex/)\n",
      "* Jul 24, 2022\n",
      "### [zer0-mistakes-bootstrap](/posts/2022-07-24-zer0-mistakes-bootstrap/)\n",
      "* Jul 16, 2022\n",
      "### [zer0-mistakes with github-pages and jekyll](/posts/2022-07-16-zer0-mistakes/)\n",
      "* Jul 1, 2022\n",
      "### [Angular Tour of Heros](/posts/2022-07-01-angular-tour-of-heros/)\n",
      "* Jun 19, 2022\n",
      "### [Webflow to Github Pages - Migration](/posts/2022-06-19-webflow-to-github-pages-migration/)\n",
      "* Jun 16, 2022\n",
      "### [GIthub Pages Custom Domain](/posts/2022-06-16-github-pages-custom-domain/)\n",
      "* Jun 10, 2022\n",
      "### [Desktop Widgets - Windows](/posts/2022-06-10-desktop-widgets-windows/)\n",
      "* Jun 4, 2022\n",
      "### [Add-Library-Category](/posts/2022-06-04-add-library-category/)\n",
      "* May 21, 2022\n",
      "### [Foundations - 000](/posts/2022-05-21-foundations-000/)\n",
      "* May 21, 2022\n",
      "### [fdsa](/posts/2022-05-21-fdsa/)\n",
      "* May 9, 2022\n",
      "### [test](/posts/js-test/)\n",
      "* Feb 27, 2022\n",
      "### [dual boot win linux](/posts/2022-02-27-dual-boot-win-linux/)\n",
      "* Feb 26, 2022\n",
      "### [android app](/posts/2022-02-26-android-app/)\n",
      "* Jan 23, 2022\n",
      "### [Winget your apps](/posts/2022-01-23-winget-your-apps/)\n",
      "* Jan 9, 2022\n",
      "### [Sonic Pi](/posts/2022-01-09-sonic-pi/)\n",
      "* Jan 7, 2022\n",
      "### [Fish Flavored Egg plant](/posts/2022-01-07-fish-flavored-egg-plant/)\n",
      "* Jan 7, 2022\n",
      "### [Jekyll and Travis](/posts/2022-01-07-jekyll-and-travis/)\n",
      "* Jan 5, 2022\n",
      "### [Angolia-Jekyll Setup With Github Pages](/posts/2022-01-05-angolia-jekyll-setup-with-github-pages/)\n",
      "* Dec 29, 2021\n",
      "### [Getting Started with Cloud Computing](/posts/2021-12-29-getting-started-with-cloud-computing/)\n",
      "* Nov 8, 2021\n",
      "### [IT Purpose](/posts/purpose/)\n",
      "* Oct 27, 2021\n",
      "### [Build-Destroy-Repeat](/posts/bdr/)\n",
      "* Sep 26, 2021\n",
      "### [Welcome to Jekyll!](/posts/2021-09-26-welcome-to-jekyll/)\n",
      "* Aug 22, 2019\n",
      "### [Auto Hide Navbar](/posts/2024-03-06-auto-hide-nav-bar/)\n",
      "* Jan 1, 2000\n",
      "### [posts](/posts/)\n",
      "* Jan 1, 2000\n",
      "### [articles](/posts/articles)\n",
      "\n",
      "subscribe [via RSS](/feed.xml)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import MarkdownConverter\n",
    "from datetime import datetime\n",
    "\n",
    "class CustomMarkdownConverter(MarkdownConverter):\n",
    "    def convert_pre(self, el, text, convert_as_inline):\n",
    "        # Extract language from class (e.g., <pre><code class=\"language-python\">)\n",
    "        language = ''\n",
    "        for child in el.find_all('code'):\n",
    "            # Extracting from class like 'language-python' or similar\n",
    "            classes = child.get('class', [])\n",
    "            for cls in classes:\n",
    "                if 'language-' in cls:\n",
    "                    language = cls.split('language-')[-1]\n",
    "                    break\n",
    "        if language:\n",
    "            return f'```{language}\\n{text.strip()}\\n```\\n'\n",
    "        return f'```\\n{text.strip()}\\n```\\n'\n",
    "\n",
    "    def convert_code(self, el, text, convert_as_inline):\n",
    "        return text  # Code elements handled in 'convert_pre'\n",
    "\n",
    "def find_content(soup):\n",
    "    content_selectors = [\n",
    "        {'tag': 'div', 'attr': {'class': 'main-content'}},\n",
    "        {'tag': 'main', 'attr': {}},\n",
    "        {'tag': 'article', 'attr': {}}\n",
    "    ]\n",
    "    for selector in content_selectors:\n",
    "        content = soup.find(selector['tag'], attrs=selector['attr'])\n",
    "        if content:\n",
    "            return content\n",
    "    return soup.find('body') or None\n",
    "\n",
    "def extract_metadata(soup, url):\n",
    "    title = soup.find('title').text if soup.find('title') else 'No Title'\n",
    "    description_tag = soup.find('meta', attrs={\"name\": \"description\"})\n",
    "    description = description_tag['content'] if description_tag else 'No Description'\n",
    "    date_retrieved = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return {\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'date_retrieved': date_retrieved,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "def clean_markdown(markdown_text):\n",
    "    lines = markdown_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    consecutive_blank_lines = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() == '':\n",
    "            consecutive_blank_lines += 1\n",
    "            if consecutive_blank_lines > 1:\n",
    "                continue\n",
    "        else:\n",
    "            consecutive_blank_lines = 0\n",
    "        \n",
    "        cleaned_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    content_div = find_content(soup)\n",
    "    metadata = extract_metadata(soup, url)\n",
    "\n",
    "    if content_div:\n",
    "        converter = CustomMarkdownConverter()\n",
    "        raw_markdown = converter.convert(str(content_div))\n",
    "        markdown_text = clean_markdown(raw_markdown)\n",
    "\n",
    "        frontmatter = f\"\"\"---\n",
    "title: \"{metadata['title']}\"\n",
    "url: \"{metadata['url']}\"\n",
    "date_retrieved: \"{metadata['date_retrieved']}\"\n",
    "description: \"{metadata['description']}\"\n",
    "---\n",
    "\"\"\"\n",
    "        return frontmatter + markdown_text\n",
    "    else:\n",
    "        return \"Content not found.\"\n",
    "\n",
    "# Example usage\n",
    "url = 'https://it-journey.dev/home/'\n",
    "markdown_output = html_to_markdown(url)\n",
    "print(markdown_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: \"ChatGPT\"\n",
      "url: \"https://chatgpt.com/share/ce7b4735-b847-488c-8321-dded8821b38a\"\n",
      "date_retrieved: \"2024-05-12 22:09:58\"\n",
      "description: \"ChatGPT is a free-to-use AI system. Use it for engaging conversations, gain insights, automate tasks, and witness the future of AI, all in one place.\"\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import MarkdownConverter\n",
    "from datetime import datetime\n",
    "\n",
    "class CustomMarkdownConverter(MarkdownConverter):\n",
    "    def convert_pre(self, el, text, convert_as_inline):\n",
    "        language = ''\n",
    "        for child in el.find_all('code'):\n",
    "            classes = child.get('class', [])\n",
    "            for cls in classes:\n",
    "                if 'language-' in cls:\n",
    "                    language = cls.split('language-')[-1]\n",
    "                    break\n",
    "        if language:\n",
    "            return f'```{language}\\n{text.strip()}\\n```\\n'\n",
    "        return f'```\\n{text.strip()}\\n```\\n'\n",
    "\n",
    "    def convert_code(self, el, text, convert_as_inline):\n",
    "        return text  # Code elements handled in 'convert_pre'\n",
    "\n",
    "def find_content(soup):\n",
    "    content_selectors = [\n",
    "        {'tag': 'div', 'attr': {'class': 'main-content'}},\n",
    "        {'tag': 'main', 'attr': {}},\n",
    "        {'tag': 'article', 'attr': {}}\n",
    "    ]\n",
    "    for selector in content_selectors:\n",
    "        content = soup.find(selector['tag'], attrs=selector['attr'])\n",
    "        if content:\n",
    "            return content\n",
    "    return soup.find('body') or None\n",
    "\n",
    "def extract_metadata(soup, url):\n",
    "    title = soup.find('title').text if soup.find('title') else 'No Title'\n",
    "    description_tag = soup.find('meta', attrs={\"name\": \"description\"})\n",
    "    description = description_tag['content'] if description_tag else 'No Description'\n",
    "    date_retrieved = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return {\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'date_retrieved': date_retrieved,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "def clean_markdown(markdown_text):\n",
    "    lines = markdown_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    consecutive_blank_lines = 0\n",
    "    previous_line = \"\"\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip() == '':\n",
    "            consecutive_blank_lines += 1\n",
    "            if consecutive_blank_lines > 1:\n",
    "                continue\n",
    "        else:\n",
    "            consecutive_blank_lines = 0\n",
    "\n",
    "        if line.strip() == '' and previous_line.strip():\n",
    "            # Check if the next line consists of = or - and convert to proper header\n",
    "            if i + 1 < len(lines) and (lines[i + 1].strip() == '=' * len(lines[i + 1].strip())):\n",
    "                cleaned_lines.append('# ' + previous_line)\n",
    "                continue\n",
    "            elif i + 1 < len(lines) and (lines[i + 1].strip() == '-' * len(lines[i + 1].strip())):\n",
    "                cleaned_lines.append('## ' + previous_line)\n",
    "                continue\n",
    "        cleaned_lines.append(line)\n",
    "        previous_line = line\n",
    "\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    content_div = find_content(soup)\n",
    "    metadata = extract_metadata(soup, url)\n",
    "\n",
    "    if content_div:\n",
    "        converter = CustomMarkdownConverter()\n",
    "        raw_markdown = converter.convert(str(content_div))\n",
    "        markdown_text = clean_markdown(raw_markdown)\n",
    "\n",
    "        frontmatter = f\"\"\"---\n",
    "title: \"{metadata['title']}\"\n",
    "url: \"{metadata['url']}\"\n",
    "date_retrieved: \"{metadata['date_retrieved']}\"\n",
    "description: \"{metadata['description']}\"\n",
    "---\n",
    "\"\"\"\n",
    "        return frontmatter + markdown_text\n",
    "    else:\n",
    "        return \"Content not found.\"\n",
    "\n",
    "# Example usage\n",
    "url = 'https://chatgpt.com/share/ce7b4735-b847-488c-8321-dded8821b38a'\n",
    "markdown_output = html_to_markdown(url)\n",
    "print(markdown_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.20.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\amrabdel-motaleb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.25.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\amrabdel-motaleb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from selenium) (2024.2.2)\n",
      "Collecting typing_extensions>=4.9.0 (from selenium)\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\amrabdel-motaleb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.20.0-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.5 MB 1.6 MB/s eta 0:00:06\n",
      "    --------------------------------------- 0.2/9.5 MB 2.4 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/9.5 MB 2.6 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.5/9.5 MB 2.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.9/9.5 MB 3.9 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/9.5 MB 4.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.7/9.5 MB 5.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/9.5 MB 5.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.4/9.5 MB 5.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.8/9.5 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.1/9.5 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.3/9.5 MB 5.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.9/9.5 MB 6.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.6/9.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/9.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.3/9.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.7/9.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.0/9.5 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.3/9.5 MB 7.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.6/9.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.0/9.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.3/9.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.6/9.5 MB 6.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.2/9.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.5/9.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.6/9.5 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.5/9.5 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 6.5 MB/s eta 0:00:00\n",
      "Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
      "   ---------------------------------------- 0.0/467.2 kB ? eta -:--:--\n",
      "   ------------------ -------------------- 225.3/467.2 kB 13.4 MB/s eta 0:00:01\n",
      "   ------------------ -------------------- 225.3/467.2 kB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 467.2/467.2 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached cffi-1.16.0-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, typing_extensions, sniffio, pysocks, pycparser, h11, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-23.2.0 cffi-1.16.0 h11-0.14.0 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 selenium-4.20.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.25.0 trio-websocket-0.11.1 typing_extensions-4.11.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmarkdownify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m markdownify \u001b[38;5;28;01mas\u001b[39;00m md\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from datetime import datetime\n",
    "\n",
    "def get_dynamic_html(url):\n",
    "    # Setup WebDriver (make sure to replace 'path_to_webdriver' with the actual path)\n",
    "    driver = webdriver.Chrome('path_to_webdriver')\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    return html\n",
    "\n",
    "def find_content(soup):\n",
    "    content_selectors = [\n",
    "        {'tag': 'div', 'attr': {'class': 'main-content'}},\n",
    "        {'tag': 'main', 'attr': {}},\n",
    "        {'tag': 'article', 'attr': {}}\n",
    "    ]\n",
    "    for selector in content_selectors:\n",
    "        content = soup.find(selector['tag'], attrs=selector['attr'])\n",
    "        if content:\n",
    "            return content\n",
    "    return None\n",
    "\n",
    "def extract_metadata(soup, url):\n",
    "    title = soup.find('title').text if soup.find('title') else 'No Title'\n",
    "    description_tag = soup.find('meta', attrs={\"name\": \"description\"})\n",
    "    description = description_tag['content'] if description_tag else 'No Description'\n",
    "    date_retrieved = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return {\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'date_retrieved': date_retrieved,\n",
    "        'description': description\n",
    "    }\n",
    "\n",
    "def clean_markdown(markdown_text):\n",
    "    return markdown_text  # Simplified for demonstration\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    html = get_dynamic_html(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    content_div = find_content(soup)\n",
    "    metadata = extract_metadata(soup, url)\n",
    "\n",
    "    if content_div:\n",
    "        markdown_text = md(str(content_div), heading_style=\"ATX\")\n",
    "        frontmatter = f\"\"\"---\n",
    "title: \"{metadata['title']}\"\n",
    "url: \"{metadata['url']}\"\n",
    "date_retrieved: \"{metadata['date_retrieved']}\"\n",
    "description: \"{metadata['description']}\"\n",
    "---\n",
    "\"\"\"\n",
    "        return frontmatter + markdown_text\n",
    "    else:\n",
    "        return \"Content not found.\"\n",
    "\n",
    "# Example usage\n",
    "url = 'https://chatgpt.com/share/ce7b4735-b847-488c-8321-dded8821b38a'  # Replace with a real URL\n",
    "markdown_output = html_to_markdown(url)\n",
    "print(markdown_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
